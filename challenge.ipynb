{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\deepsort\\deep_sort\\packages/deep-person-reid\\torchreid\\metrics\\rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from deep_sort_app import run\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_files = [\n",
    "    \"testing_data/KITTI-17/\",\n",
    "    \"testing_data/MOT16-09/\",\n",
    "    \"testing_data/MOT16-11/\",\n",
    "    \"testing_data/PETS09-S2L1/\",\n",
    "    \"testing_data/TUD-Campus/\",\n",
    "    \"testing_data/TUD-Stadtmitte/\",\n",
    "]\n",
    "detection_models = [\n",
    "    \"original\",\n",
    "    \"yolov5nano\",\n",
    "    # \"yolov5small\",\n",
    "    # \"yolov5medium\",\n",
    "    # \"yolov5large\",\n",
    "    \"yolov5extralarge\",\n",
    "    \"yolov10nano\",\n",
    "    # \"yolov10small\",\n",
    "    # \"yolov10medium\",\n",
    "    \"yolov10balanced\",\n",
    "    # \"yolov10large\",\n",
    "    \"yolov10extralarge\",\n",
    "    \"nanodet\",\n",
    "]\n",
    "feature_generator_models = [\n",
    "    \"original\",\n",
    "    \"dpreid_shufflenet\",\n",
    "    \"dpreid_mlfn\",\n",
    "    \"dpreid_mobilenetv2_1_0\",\n",
    "    # \"dpreid_mobilenetv2_1_4\",\n",
    "    \"dpreid_osnet_ibn_x1_0\",\n",
    "    \"dpreid_osnet_ain_x1_0\",\n",
    "    # \"dpreid_osnet_ain_x0_75\",\n",
    "    # \"dpreid_osnet_ain_x0_5\",\n",
    "    # \"dpreid_osnet_ain_x0_25\",\n",
    "    \"dpreid_osnet_x1_0\",\n",
    "    # \"dpreid_osnet_x0_75\",\n",
    "    # \"dpreid_osnet_x0_5\",\n",
    "    # \"dpreid_osnet_x0_25\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"min_confidence\": 0.8,\n",
    "    \"nms_max_overlap\": 1.0,\n",
    "    \"min_detection_height\": 0,\n",
    "    \"max_cosine_distance\": 0.2,\n",
    "    \"nn_budget\": None\n",
    "}\n",
    "display = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original original\n",
      "WARNING:tensorflow:From e:\\Projects\\deepsort\\deep_sort\\deep_sort\\feature_generators\\image_encoder.py:10: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "testing_data/KITTI-17/ {'precision': 0.9599271402550091, 'recall': 0.7715959004392386, 'f1-score': 0.8555194805194806, 'hota': 0.8084988883560865, 'frames': 145, 'fps': 40.23911868207499}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\deepsort\\deep_sort\\deep_sort\\preprocessing.py:47: RuntimeWarning: overflow encountered in multiply\n",
      "  area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
      "e:\\Projects\\deepsort\\deep_sort\\deep_sort\\preprocessing.py:66: RuntimeWarning: overflow encountered in multiply\n",
      "  overlap = (w * h) / area[idxs[:last]]\n",
      "e:\\Projects\\deepsort\\deep_sort\\deep_sort\\preprocessing.py:66: RuntimeWarning: invalid value encountered in divide\n",
      "  overlap = (w * h) / area[idxs[:last]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_data/MOT16-09/ {'precision': 1.0, 'recall': 0.387102910405174, 'f1-score': 0.5581459133296763, 'hota': 0.6196377194614634, 'frames': 525, 'fps': 21.813611408314493}\n",
      "testing_data/MOT16-11/ {'precision': 0.9997514292816306, 'recall': 0.43841290603880534, 'f1-score': 0.6095324695006442, 'hota': 0.6574320796793435, 'frames': 900, 'fps': 17.879135996468424}\n",
      "testing_data/PETS09-S2L1/ {'precision': 0.7955116696588869, 'recall': 0.989946380697051, 'f1-score': 0.882142146127812, 'hota': 0.8945779441853692, 'frames': 795, 'fps': 20.199501897678903}\n",
      "testing_data/TUD-Campus/ {'precision': 0.9314641744548287, 'recall': 0.8328690807799443, 'f1-score': 0.8794117647058822, 'hota': 0.824727597721162, 'frames': 71, 'fps': 63.50570418911418}\n",
      "testing_data/TUD-Stadtmitte/ {'precision': 0.9201419698314108, 'recall': 0.8970588235294118, 'f1-score': 0.9084537888742883, 'hota': 0.8912429021703114, 'frames': 179, 'fps': 37.123901335269025}\n",
      "{'precision': 0.93, 'recall': 0.72, 'f1-score': 0.78, 'hota': 0.78, 'fps': 33.46}\n",
      "original dpreid_shufflenet\n",
      "Model: shufflenet\n",
      "- params: 904,728\n",
      "- flops: 89,143,296\n",
      "Successfully loaded pretrained weights from \"models/feature_generation/deeppersonreid/shufflenet.pth.tar\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.bias', 'classifier.weight']\n",
      "testing_data/KITTI-17/ {'precision': 0.9599271402550091, 'recall': 0.7715959004392386, 'f1-score': 0.8555194805194806, 'hota': 0.8200428779071118, 'frames': 145, 'fps': 36.56263406469734}\n",
      "Model: shufflenet\n",
      "- params: 904,728\n",
      "- flops: 89,143,296\n",
      "Successfully loaded pretrained weights from \"models/feature_generation/deeppersonreid/shufflenet.pth.tar\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.bias', 'classifier.weight']\n",
      "testing_data/MOT16-09/ {'precision': 1.0, 'recall': 0.387102910405174, 'f1-score': 0.5581459133296763, 'hota': 0.6278084552449694, 'frames': 525, 'fps': 5.5963691972200955}\n",
      "Model: shufflenet\n",
      "- params: 904,728\n",
      "- flops: 89,143,296\n",
      "Successfully loaded pretrained weights from \"models/feature_generation/deeppersonreid/shufflenet.pth.tar\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.bias', 'classifier.weight']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m mot_files:\n\u001b[0;32m     20\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 21\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin_confidence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnms_max_overlap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin_detection_height\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_cosine_distance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnn_budget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfg_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplay\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     35\u001b[0m     metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m (t1 \u001b[38;5;241m-\u001b[39m t0)\n",
      "File \u001b[1;32me:\\Projects\\deepsort\\deep_sort\\deep_sort_app.py:325\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(sequence_dir, detection_file, output_file, min_confidence, nms_max_overlap, min_detection_height, max_cosine_distance, nn_budget, detection_model, feature_generator_model, display)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     visualizer \u001b[38;5;241m=\u001b[39m visualization\u001b[38;5;241m.\u001b[39mNoVisualization(seq_info)\n\u001b[1;32m--> 325\u001b[0m \u001b[43mvisualizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m classic_metrics \u001b[38;5;241m=\u001b[39m ClassicsMetric(metric_detections, metric_gts)\n\u001b[0;32m    329\u001b[0m metrics \u001b[38;5;241m=\u001b[39m classic_metrics\u001b[38;5;241m.\u001b[39mget_metric()\n",
      "File \u001b[1;32me:\\Projects\\deepsort\\deep_sort\\deep_sort\\visualization.py:79\u001b[0m, in \u001b[0;36mNoVisualization.run\u001b[1;34m(self, frame_callback)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, frame_callback):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_idx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_idx:\n\u001b[1;32m---> 79\u001b[0m         \u001b[43mframe_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32me:\\Projects\\deepsort\\deep_sort\\deep_sort_app.py:282\u001b[0m, in \u001b[0;36mrun.<locals>.frame_callback\u001b[1;34m(vis, frame_idx)\u001b[0m\n\u001b[0;32m    278\u001b[0m detections \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mget_detections(seq_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_filenames\u001b[39m\u001b[38;5;124m\"\u001b[39m][frame_idx], min_detection_height)\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# detections = create_detections(\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m#     seq_info[\"detections\"], frame_idx, min_detection_height)\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage_filenames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtlwh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m detections \u001b[38;5;241m=\u001b[39m [Detection(detection\u001b[38;5;241m.\u001b[39mtlwh, detection\u001b[38;5;241m.\u001b[39mconfidence, feature) \u001b[38;5;28;01mfor\u001b[39;00m detection, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(detections, features)]\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m# detections = create_detections(detections_all, frame_idx, 0)\u001b[39;00m\n",
      "File \u001b[1;32me:\\Projects\\deepsort\\deep_sort\\deep_sort\\feature_generators\\dpreid_fg.py:117\u001b[0m, in \u001b[0;36mDeepPersonReidFG.get_features\u001b[1;34m(self, image_file, boxes)\u001b[0m\n\u001b[0;32m    115\u001b[0m     image_patches\u001b[38;5;241m.\u001b[39mappend(patch)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# image_patches = np.asarray(image_patches)\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_patches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32me:\\Projects\\deepsort\\deep_sort\\packages/deep-person-reid\\torchreid\\utils\\feature_extractor.py:125\u001b[0m, in \u001b[0;36mFeatureExtractor.__call__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    122\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mType of each element must belong to [str | numpy.ndarray]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    123\u001b[0m         )\n\u001b[1;32m--> 125\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[0;32m    128\u001b[0m images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(images, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32me:\\Projects\\deepsort\\deep_sort\\venvgpu\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32me:\\Projects\\deepsort\\deep_sort\\venvgpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects\\deepsort\\deep_sort\\venvgpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Projects\\deepsort\\deep_sort\\venvgpu\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects\\deepsort\\deep_sort\\venvgpu\\Lib\\site-packages\\torchvision\\transforms\\functional.py:468\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    466\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    467\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32me:\\Projects\\deepsort\\deep_sort\\venvgpu\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects\\deepsort\\deep_sort\\venvgpu\\Lib\\site-packages\\PIL\\Image.py:2200\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2192\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2193\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2194\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2195\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2196\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2197\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2198\u001b[0m         )\n\u001b[1;32m-> 2200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"results_original_hyperparameters.md\", \"a\") as file:\n",
    "    for d_model, fg_model in ((x, y) for x in detection_models for y in feature_generator_models):\n",
    "        mean_metrics = {\n",
    "            \"precision\": 0,\n",
    "            \"recall\": 0,\n",
    "            \"f1-score\": 0,\n",
    "            \"hota\": 0,\n",
    "            \"fps\": 0\n",
    "        }\n",
    "        print(d_model, fg_model)\n",
    "        file.write(\"Detection Model: \" + d_model + \"\\n\\n\")\n",
    "        file.write(\"Feature Generation Model: \" + fg_model + \"\\n\\n\")\n",
    "        video_ = []\n",
    "        precision_ = []\n",
    "        recall_ = []\n",
    "        f1_score_ = []\n",
    "        hota_ = []\n",
    "        fps_ = []\n",
    "        for seq in mot_files:\n",
    "            t0 = time.time()\n",
    "            metrics = run(\n",
    "                seq, \n",
    "                None, \n",
    "                None, \n",
    "                hyperparameters[\"min_confidence\"],\n",
    "                hyperparameters[\"nms_max_overlap\"],\n",
    "                hyperparameters[\"min_detection_height\"],\n",
    "                hyperparameters[\"max_cosine_distance\"],\n",
    "                hyperparameters[\"nn_budget\"],\n",
    "                d_model,\n",
    "                fg_model,\n",
    "                display\n",
    "            )\n",
    "            t1 = time.time()\n",
    "            metrics[\"fps\"] = metrics[\"frames\"] / (t1 - t0)\n",
    "            print(seq, metrics)\n",
    "            # file.write(\"Video: \" + seq + \"\\n\")\n",
    "            # file.write(\"Precision: \" + str(round(metrics[\"precision\"], 2)) + \"\\n\")\n",
    "            # file.write(\"Recall: \" + str(round(metrics[\"recall\"], 2)) + \"\\n\")\n",
    "            # file.write(\"F1-score: \" + str(round(metrics[\"f1-score\"], 2)) + \"\\n\")\n",
    "            # file.write(\"Hota: \" + str(round(metrics[\"hota\"], 2)) + \"\\n\") \n",
    "            # file.write(\"FPS: \" + str(round(metrics[\"fps\"], 2)) + \"\\n\") \n",
    "            video_.append(seq)\n",
    "            precision_.append(str(round(metrics[\"precision\"], 2)))\n",
    "            recall_.append(str(round(metrics[\"recall\"], 2)))\n",
    "            f1_score_.append(str(round(metrics[\"f1-score\"], 2)))\n",
    "            hota_.append(str(round(metrics[\"hota\"], 2)))\n",
    "            fps_.append(str(round(metrics[\"fps\"], 2)))\n",
    "            mean_metrics[\"precision\"] += metrics[\"precision\"]\n",
    "            mean_metrics[\"recall\"] += metrics[\"recall\"]\n",
    "            mean_metrics[\"f1-score\"] += metrics[\"f1-score\"]\n",
    "            mean_metrics[\"hota\"] += metrics[\"hota\"]\n",
    "            mean_metrics[\"fps\"] += metrics[\"fps\"]\n",
    "        mean_metrics[\"precision\"] = round(mean_metrics[\"precision\"] / len(mot_files), 2)\n",
    "        mean_metrics[\"recall\"] = round(mean_metrics[\"recall\"] / len(mot_files), 2)\n",
    "        mean_metrics[\"f1-score\"] = round(mean_metrics[\"f1-score\"] / len(mot_files), 2)\n",
    "        mean_metrics[\"hota\"] = round(mean_metrics[\"hota\"] / len(mot_files), 2)\n",
    "        mean_metrics[\"fps\"] = round(mean_metrics[\"fps\"] / len(mot_files), 2)\n",
    "        file.write(\"Metrics Table:\\n\\n\")\n",
    "        file.write(\"| Video | Precision | Recall | F1-score | HOTA | FPS |\\n\")\n",
    "        file.write(\"| --- | --- | --- | --- | --- | --- |\\n\")\n",
    "        for i in range(len(video_)):\n",
    "            file.write(\"| \" + video_[i] + \" | \" + precision_[i] + \" | \" + recall_[i] + \" | \" + f1_score_[i] + \" | \" + hota_[i] + \" | \" + fps_[i] + \" |\\n\")\n",
    "        file.write(\"| ALL | \" + str(mean_metrics[\"precision\"]) + \" | \" + str(mean_metrics[\"recall\"]) + \" | \" + str(mean_metrics[\"f1-score\"]) + \" | \" + str(mean_metrics[\"hota\"]) + \" | \" + str(mean_metrics[\"fps\"]) + \" |\\n\")\n",
    "        file.write(\"---\\n\")\n",
    "\n",
    "        # file.write(\"Mean metrics for all videos\\n\")\n",
    "        # file.write(\"Precision: \" + str(round(mean_metrics[\"precision\"], 2)) + \"\\n\")\n",
    "        # file.write(\"Recall: \" + str(round(mean_metrics[\"recall\"], 2)) + \"\\n\")\n",
    "        # file.write(\"F1-score: \" + str(round(mean_metrics[\"f1-score\"], 2)) + \"\\n\")\n",
    "        # file.write(\"Hota: \" + str(round(mean_metrics[\"hota\"], 2)) + \"\\n\") \n",
    "        # file.write(\"FPS: \" + str(round(mean_metrics[\"fps\"], 2)) + \"\\n\") \n",
    "        # file.write(\"---------\\n\")\n",
    "\n",
    "        print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
